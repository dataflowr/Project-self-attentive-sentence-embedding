{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scenic-stuart",
   "metadata": {},
   "source": [
    "# Model analysis and heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continental-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "confident-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj 7.9169617\n",
      "is AUX aux 8.597497\n",
      "looking VERB ROOT 8.68382\n",
      "at ADP prep 7.737509\n",
      "buying VERB pcomp 9.099992\n",
      "U.K. PROPN dobj 7.7217307\n",
      "startup NOUN advcl 5.9194016\n",
      "for ADP prep 7.6531463\n",
      "$ SYM quantmod 9.727607\n",
      "1 NUM compound 10.960313\n",
      "billion NUM pobj 8.402847\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "documentary-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "senior-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sublime-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Function that prints a the summary table of a network. \n",
    "        It also counts how many parameters require gradient.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): A neural network with multiple paramters requiring gradient.\n",
    "\n",
    "    Returns:\n",
    "        int: Returns the total number of parameters that requires gradient. \n",
    "    \"\"\"\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "            \n",
    "        ## Splitting the name into processable data:\n",
    "        l        = name.split(\".\")\n",
    "\n",
    "        Net_name = l[0]\n",
    "        fun      = l[1]\n",
    "        \n",
    "                    \n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "needed-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bestval = torch.load(\"models/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pointed-enclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Classifier(\n",
       "  (encoder): SelfAttentiveEncoder(\n",
       "    (bilstm): BiLSTM(\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (encoder): Embedding(28199, 200)\n",
       "      (bilstm): LSTM(200, 300, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "    )\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (ws1): Linear(in_features=600, out_features=350, bias=False)\n",
       "    (ws2): Linear(in_features=350, out_features=4, bias=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (fc): Linear(in_features=2400, out_features=300, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (pred): Linear(in_features=300, out_features=5, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bestval.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chemical-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+------------+\n",
      "|                  Modules                   | Parameters |\n",
      "+--------------------------------------------+------------+\n",
      "|       encoder.bilstm.encoder.weight        |  5639800   |\n",
      "|     encoder.bilstm.bilstm.weight_ih_l0     |   240000   |\n",
      "|     encoder.bilstm.bilstm.weight_hh_l0     |   360000   |\n",
      "|      encoder.bilstm.bilstm.bias_ih_l0      |    1200    |\n",
      "|      encoder.bilstm.bilstm.bias_hh_l0      |    1200    |\n",
      "| encoder.bilstm.bilstm.weight_ih_l0_reverse |   240000   |\n",
      "| encoder.bilstm.bilstm.weight_hh_l0_reverse |   360000   |\n",
      "|  encoder.bilstm.bilstm.bias_ih_l0_reverse  |    1200    |\n",
      "|  encoder.bilstm.bilstm.bias_hh_l0_reverse  |    1200    |\n",
      "|     encoder.bilstm.bilstm.weight_ih_l1     |   720000   |\n",
      "|     encoder.bilstm.bilstm.weight_hh_l1     |   360000   |\n",
      "|      encoder.bilstm.bilstm.bias_ih_l1      |    1200    |\n",
      "|      encoder.bilstm.bilstm.bias_hh_l1      |    1200    |\n",
      "| encoder.bilstm.bilstm.weight_ih_l1_reverse |   720000   |\n",
      "| encoder.bilstm.bilstm.weight_hh_l1_reverse |   360000   |\n",
      "|  encoder.bilstm.bilstm.bias_ih_l1_reverse  |    1200    |\n",
      "|  encoder.bilstm.bilstm.bias_hh_l1_reverse  |    1200    |\n",
      "|             encoder.ws1.weight             |   210000   |\n",
      "|             encoder.ws2.weight             |    1400    |\n",
      "|                 fc.weight                  |   720000   |\n",
      "|                  fc.bias                   |    300     |\n",
      "|                pred.weight                 |    1500    |\n",
      "|                 pred.bias                  |     5      |\n",
      "+--------------------------------------------+------------+\n",
      "Total Trainable Params: 9942605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9942605"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_bestval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "endangered-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bestacc = torch.load(\"models/test..best_acc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "surface-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Classifier(\n",
       "  (encoder): SelfAttentiveEncoder(\n",
       "    (bilstm): BiLSTM(\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (encoder): Embedding(28199, 200)\n",
       "      (bilstm): LSTM(200, 300, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "    )\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (ws1): Linear(in_features=600, out_features=350, bias=False)\n",
       "    (ws2): Linear(in_features=350, out_features=4, bias=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (fc): Linear(in_features=2400, out_features=300, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (pred): Linear(in_features=300, out_features=5, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/Data/pls_do_not_delete/Project-self-attentive-sentence-embedding/small/test_tok.json\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
